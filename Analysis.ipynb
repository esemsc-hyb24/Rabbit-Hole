{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb540141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/iga224/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP & ML\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "\n",
    "# Sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Utilities\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50075b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 100 rows, 5 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the actual CSV\n",
    "df = pd.read_csv(\"dummy_data.csv\")\n",
    "\n",
    "# Rename columns for consistency with pipeline\n",
    "df = df.rename(columns={\n",
    "    \"visit_time\": \"timestamp\",\n",
    "    \"page_text\": \"text\"\n",
    "})\n",
    "\n",
    "# Ensure timestamp is datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "# Fill missing titles (optional: fallback to URL path or empty string)\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "\n",
    "# Drop rows with unusable text (e.g. full error messages or NaNs)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df = df[~df[\"text\"].str.contains(\"404 Client Error\", na=False)]\n",
    "df = df[df[\"text\"].str.strip().astype(bool)]\n",
    "#  crop df to first 100 rows \n",
    "df = df.head(100)\n",
    "\n",
    "# Display preview\n",
    "df[[\"timestamp\", \"url\", \"title\", \"text\"]].head()\n",
    "\n",
    "#print size of the DataFrame\n",
    "print(f\"DataFrame size: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4af7f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 13/13 [01:02<00:00,  4.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load E5 model and tokenizer\n",
    "model_name = 'intfloat/multilingual-e5-large-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Format inputs for E5 (prepend instruction)\n",
    "df[\"formatted_text\"] = df[\"text\"].apply(lambda x: f\"query: {x}\")\n",
    "\n",
    "# Pooling function\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Encode in batches\n",
    "batch_size = 8\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = df[\"formatted_text\"].iloc[i:i+batch_size].tolist()\n",
    "        tokens = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "        output = model(**tokens)\n",
    "        embeddings = average_pool(output.last_hidden_state, tokens['attention_mask'])\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "\n",
    "# Combine into single array\n",
    "embedding_matrix = torch.cat(all_embeddings)\n",
    "df[\"embedding\"] = list(embedding_matrix.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ce4b323",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: UMAP dimensionality reduction to 15 dimensions\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m umap_reducer \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP(\n\u001b[1;32m      6\u001b[0m     n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      7\u001b[0m     min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      9\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m embedding_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     14\u001b[0m embedding_15d \u001b[38;5;241m=\u001b[39m umap_reducer\u001b[38;5;241m.\u001b[39mfit_transform(embedding_array)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap \n",
    "import hdbscan\n",
    "\n",
    "# Step 1: UMAP dimensionality reduction to 15 dimensions\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    n_components=15,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embedding_array = np.vstack(df[\"embedding\"].values)\n",
    "embedding_15d = umap_reducer.fit_transform(embedding_array)\n",
    "\n",
    "# Step 2: HDBSCAN clustering\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=3,\n",
    "    metric='euclidean',\n",
    "    prediction_data=True,\n",
    "    gen_min_span_tree=True\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embedding_15d)\n",
    "\n",
    "# Step 3: Assign cluster labels to DataFrame\n",
    "df[\"cluster\"] = cluster_labels\n",
    "\n",
    "# Step 4: Print clustering stats\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = np.sum(cluster_labels == -1)\n",
    "\n",
    "print(f\"✅ Clustering Complete\")\n",
    "print(f\"Total clusters: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f15da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/iga224/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP & ML\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "\n",
    "# Sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Utilities\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1618a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cluster'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(records)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m---> 37\u001b[0m cluster_summary \u001b[38;5;241m=\u001b[39m extract_top_keywords_c_tf_idf(df)\n\u001b[1;32m     38\u001b[0m display(cluster_summary)\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mextract_top_keywords_c_tf_idf\u001b[0;34m(df, text_col, label_col, top_k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_top_keywords_c_tf_idf\u001b[39m(df, text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Combine texts per cluster\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cluster_texts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 6\u001b[0m         df[df[label_col] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# exclude noise\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupby(label_col)[text_col]\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m texts: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(texts))\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Count Vectorizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     count_vec \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cluster'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def extract_top_keywords_c_tf_idf(df, text_col='text', label_col='cluster', top_k=10):\n",
    "    # Combine texts per cluster\n",
    "    cluster_texts = (\n",
    "        df[df[label_col] != -1]  # exclude noise\n",
    "        .groupby(label_col)[text_col]\n",
    "        .apply(lambda texts: \" \".join(texts))\n",
    "    )\n",
    "\n",
    "    # Count Vectorizer\n",
    "    count_vec = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    count_matrix = count_vec.fit_transform(cluster_texts)\n",
    "\n",
    "    # Class-based TF-IDF\n",
    "    tfidf = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    ctfidf_matrix = tfidf.fit_transform(count_matrix)\n",
    "\n",
    "    # Extract keywords and build results\n",
    "    feature_names = count_vec.get_feature_names_out()\n",
    "    records = []\n",
    "    for idx, row in enumerate(ctfidf_matrix):\n",
    "        cluster_id = cluster_texts.index[idx]\n",
    "        sorted_indices = row.toarray().flatten().argsort()[::-1][:top_k]\n",
    "        keywords = [feature_names[i] for i in sorted_indices]\n",
    "        count = df[df[label_col] == cluster_id].shape[0]\n",
    "        records.append({\n",
    "            \"cluster\": cluster_id,\n",
    "            \"keywords\": \", \".join(keywords),\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "    results = pd.DataFrame(records).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "    return results\n",
    "\n",
    "cluster_summary = extract_top_keywords_c_tf_idf(df)\n",
    "display(cluster_summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aedb9757",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cluster'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     summary_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_df\n\u001b[0;32m---> 39\u001b[0m cluster_summary \u001b[38;5;241m=\u001b[39m extract_top_keywords_c_tf_idf(df)\n\u001b[1;32m     40\u001b[0m cluster_summary \u001b[38;5;241m=\u001b[39m label_clusters_with_ollama_df(cluster_summary)\n\u001b[1;32m     41\u001b[0m display(cluster_summary)\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mextract_top_keywords_c_tf_idf\u001b[0;34m(df, text_col, label_col, top_k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_top_keywords_c_tf_idf\u001b[39m(df, text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Combine texts per cluster\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cluster_texts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 6\u001b[0m         df[df[label_col] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# exclude noise\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupby(label_col)[text_col]\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m texts: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(texts))\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Count Vectorizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     count_vec \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cluster'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def label_clusters_with_ollama_df(summary_df, model_name=\"gemma3:4b\"):\n",
    "    \"\"\"\n",
    "    Add a 'label' column to a cluster summary DataFrame using a local Ollama model via LangChain.\n",
    "\n",
    "    Args:\n",
    "        summary_df (pd.DataFrame): DataFrame with 'cluster' and 'keywords' columns.\n",
    "        model_name (str): Name of the Ollama model to use (default: 'gemma3:4b').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an added 'label' column.\n",
    "    \"\"\"\n",
    "    llm = Ollama(model=model_name)\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"You are helping categorize clusters of search history topics.\\n\"\n",
    "        \"Given the following top keywords, generate a short and specific label \"\n",
    "        \"that summarizes the main idea of the cluster using 2–4 words. Avoid vague or generic terms.\\n\\n\"\n",
    "        \"Keywords: {keywords}\\n\"\n",
    "        \"Label:\"\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for _, row in summary_df.iterrows():\n",
    "        prompt = prompt_template.format(keywords=row[\"keywords\"])\n",
    "        try:\n",
    "            label = llm.invoke(prompt).strip()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating label for cluster {row['cluster']}: {e}\")\n",
    "            label = \"Unknown\"\n",
    "        labels.append(label)\n",
    "\n",
    "    summary_df = summary_df.copy()\n",
    "    summary_df[\"label\"] = labels\n",
    "    return summary_df\n",
    "\n",
    "cluster_summary = extract_top_keywords_c_tf_idf(df)\n",
    "cluster_summary = label_clusters_with_ollama_df(cluster_summary)\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45758a9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: UMAP to 2D\u001b[39;00m\n\u001b[1;32m      6\u001b[0m umap_2d \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m embedding_2d \u001b[38;5;241m=\u001b[39m umap_2d\u001b[38;5;241m.\u001b[39mfit_transform(embedding_array)\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_2d[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_2d[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_array' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Step 1: UMAP to 2D\n",
    "umap_2d = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', random_state=42)\n",
    "embedding_2d = umap_2d.fit_transform(embedding_array)\n",
    "\n",
    "df[\"x_2d\"] = embedding_2d[:, 0]\n",
    "df[\"y_2d\"] = embedding_2d[:, 1]\n",
    "\n",
    "# Step 2: Create cluster-to-label map from the cluster summary DataFrame\n",
    "label_map = dict(zip(cluster_summary[\"cluster\"], cluster_summary[\"label\"]))\n",
    "df[\"cluster_label\"] = df[\"cluster\"].apply(lambda cid: label_map.get(cid, \"Noise\") if cid != -1 else \"Noise\")\n",
    "\n",
    "# Step 3: Color palette\n",
    "unique_clusters = sorted(df[\"cluster\"].unique())\n",
    "palette = sns.color_palette(\"husl\", len(unique_clusters))\n",
    "cluster_color_map = {cid: palette[i] for i, cid in enumerate(unique_clusters)}\n",
    "\n",
    "# Step 4: Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cid in unique_clusters:\n",
    "    subset = df[df[\"cluster\"] == cid]\n",
    "    label = label_map.get(cid, \"Noise\") if cid != -1 else \"Noise\"\n",
    "    plt.scatter(subset[\"x_2d\"], subset[\"y_2d\"], s=60, label=label, color=cluster_color_map[cid], alpha=0.7)\n",
    "\n",
    "plt.title(\"Search History Clusters (UMAP 2D)\", fontsize=14)\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"Cluster Labels\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04063c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'topic_title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'topic_title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m sentiment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInfo: NEUTRAL sentiment dominates \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(percentage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of all keywords.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m clusters \u001b[38;5;241m=\u001b[39m extract_clusters_from_df(df)\n\u001b[1;32m     85\u001b[0m all_keywords \u001b[38;5;241m=\u001b[39m extract_all_keywords(clusters)\n\u001b[1;32m     86\u001b[0m keyword_sentiments \u001b[38;5;241m=\u001b[39m classify_all_keywords(all_keywords)\n",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mextract_clusters_from_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m clusters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m     cluster \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_title\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# now last column\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: [kw\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_points\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_points\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     10\u001b[0m     clusters\u001b[38;5;241m.\u001b[39mappend(cluster)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clusters\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mpm2024/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'topic_title'"
     ]
    }
   ],
   "source": [
    "# === Step 1: Parse cluster topics and keywords ===\n",
    "def extract_clusters_from_df(df):\n",
    "    clusters = []\n",
    "    for _, row in df.iterrows():\n",
    "        cluster = {\n",
    "            \"topic\": row[\"topic_title\"],  # now last column\n",
    "            \"keywords\": [kw.strip().lower() for kw in row[\"keywords\"].split(\",\")],\n",
    "            \"num_points\": row[\"num_points\"]\n",
    "        }\n",
    "        clusters.append(cluster)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# === Step 2: Extract all keywords across all clusters ===\n",
    "def extract_all_keywords(clusters):\n",
    "    return [kw for cluster in clusters for kw in cluster[\"keywords\"]]\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_word_sentiment(word):\n",
    "    score = analyser.polarity_scores(word)['compound']\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def classify_all_keywords(keywords):\n",
    "    return [classify_word_sentiment(word) for word in keywords]\n",
    "\n",
    "\n",
    "# === Step 4: Plot sentiment pie chart ===\n",
    "def plot_keyword_sentiment_pie(sentiments):\n",
    "    counts = Counter(sentiments)\n",
    "    labels = counts.keys()\n",
    "    sizes = counts.values()\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=[\"green\", \"red\", \"grey\"])\n",
    "    plt.title(\"Sentiment Breakdown of Keywords\")\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Step 5: Plot pie chart of topics ===\n",
    "def plot_topic_names(clusters):\n",
    "    topic_counts = {c[\"topic\"]: c[\"num_points\"] for c in clusters}\n",
    "    sorted_topics = dict(sorted(topic_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.pie(sorted_topics.values(), labels=sorted_topics.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(\"Cluster Topic Distribution (by # of Points)\")\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Step 6: Bubble Detection (based on keyword sentiment) ===\n",
    "def detect_sentiment_bubble(sentiments, threshold=0.7):\n",
    "    counts = Counter(sentiments)\n",
    "    total = len(sentiments)\n",
    "    for sentiment, count in counts.items():\n",
    "        if count / total > threshold:\n",
    "            print(f\"\\n Sentiment Bubble Detected: '{sentiment.upper()}' dominates {int(count / total * 100)}% of all keywords.\")\n",
    "\n",
    "# === Step 7: Detect sentiment warnings ===\n",
    "def detect_sentiment_warnings(sentiments, threshold=0.6):\n",
    "    counts = Counter(sentiments)\n",
    "    total = len(sentiments)\n",
    "\n",
    "    for sentiment, count in counts.items():\n",
    "        percentage = count / total\n",
    "        if percentage > threshold:\n",
    "            if sentiment == \"negative\":\n",
    "                print(f\"\\nWarning: NEGATIVE sentiment dominates {int(percentage * 100)}% of all keywords.\")\n",
    "            elif sentiment == \"positive\":\n",
    "                print(f\"\\nHeads up: POSITIVE sentiment dominates {int(percentage * 100)}% of all keywords.\")\n",
    "            elif sentiment == \"neutral\":\n",
    "                print(f\"\\nInfo: NEUTRAL sentiment dominates {int(percentage * 100)}% of all keywords.\")\n",
    "\n",
    "\n",
    "clusters = extract_clusters_from_df(df)\n",
    "all_keywords = extract_all_keywords(clusters)\n",
    "keyword_sentiments = classify_all_keywords(all_keywords)\n",
    "\n",
    "plot_topic_names(clusters)                  # weighted by num_points\n",
    "plot_keyword_sentiment_pie(keyword_sentiments)  # pie chart\n",
    "detect_sentiment_bubble(keyword_sentiments)\n",
    "detect_sentiment_warnings(keyword_sentiments)\n",
    "\n",
    "print(\"\\nKeyword Sentiment Summary:\")\n",
    "summary = Counter(keyword_sentiments)\n",
    "for s, count in summary.items():\n",
    "    print(f\"{s.capitalize()}: {count} keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede8029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cedde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpm2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
